{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=requests.get('https://www.naver.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=requests.get('https://search.naver.com/search.naver', params={\n",
    "    'where':'news',\n",
    "    'query':'설날'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup= BeautifulSoup(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_real = soup.find('ol', class_='lst_realtime_srch _tab_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_item =news_real.find_all('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_item[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_item_text = []\n",
    "for news in news_item:\n",
    "    news_item_text.append(news.text)\n",
    "print(news_item_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#숫자 빼기\n",
    "news_item = news_real.find_all('span', class_='tit')\n",
    "\n",
    "news_item_text =[]\n",
    "\n",
    "for news in news_item:\n",
    "    news_item_text.append(news.text)\n",
    "    \n",
    "print(news_item_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_item = news_real.select('ol.lst_realtime_srch span.tit')\n",
    "print(news_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 펭수 뉴스 검색하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.url 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://search.naver.com/search.naver'\n",
    "params={\n",
    "    'where': 'news',\n",
    "    'query': '펭수'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.request 보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp =requests.get(url, params=params)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.resp 분석하기(soup객체 만들기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ul_tag=soup.find('ul', class_='type01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_item_tags=ul_tag.find_all('a', class_='_sp_each_title')\n",
    "len(news_item_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_item_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list =[]\n",
    "for news in news_item_tags:\n",
    "    title = news.text\n",
    "    link =news.get('href')\n",
    "    news_list.append(\n",
    "        {\n",
    "            'title':title,\n",
    "            'link':link\n",
    "        }\n",
    "    )\n",
    "news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_tags =ul_tag.find_all('li', recursive=False)\n",
    "len(li_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for li_tag in li_tags:\n",
    "    a_tag = li_tag.find('a', class_='_sp_each_title')\n",
    "    title = a_tag.text\n",
    "    link = a_tag.get('href')\n",
    "    \n",
    "    dd_tag = li_tag.find_all('dd')[1]\n",
    "    desc = dd_tag.text\n",
    "    \n",
    "    news_list.append({\n",
    "        'title':title,\n",
    "        'link':link,\n",
    "        'desc':desc\n",
    "    })\n",
    "news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 블로그 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://search.naver.com/search.naver' \n",
    "params={\n",
    "    'where': 'post',\n",
    "    'query': '아이폰 11 pro'\n",
    "}\n",
    "\n",
    "resp =requests.get(url, params=params)\n",
    "resp\n",
    "\n",
    "soup = BeautifulSoup(resp.content)\n",
    "\n",
    "ul_tag=soup.find('ul', class_='type01')\n",
    "\n",
    "blog_item_tags=ul_tag.find_all('a', class_='sh_blog_title _sp_each_url _sp_each_title')\n",
    "len(blog_item_tags)\n",
    "\n",
    "blog_item_tags[0]\n",
    "\n",
    "blog_list =[]\n",
    "\n",
    "\n",
    "li_tags =ul_tag.find_all('li', recursive=False)\n",
    "len(li_tags)\n",
    "\n",
    "for li_tag in li_tags:\n",
    "    a_tag = li_tag.find('a', class_='sh_blog_title _sp_each_url _sp_each_title')\n",
    "    title = a_tag.text\n",
    "    link = a_tag.get('href')\n",
    "    \n",
    "    dd_tag = li_tag.find_all('dd')[1]\n",
    "    desc = dd_tag.text\n",
    "    \n",
    "    blog_list.append({\n",
    "        'title':title,\n",
    "        'link':link,\n",
    "        'desc':desc\n",
    "    })\n",
    "blog_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연습문제 2\n",
    "-연관검색어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url ='https://search.naver.com/search.naver' \n",
    "params={\n",
    "    'where': 'post',\n",
    "    'query': '무역전쟁'\n",
    "}\n",
    "resp =requests.get(url, params=params)\n",
    "\n",
    "soup = BeautifulSoup(resp.content)\n",
    "\n",
    "ul_tag = soup.find('ul', class_='_related_keyword_ul')\n",
    "#dl_tag = soup.find('dl', class_='relate_area _related_keyword_area relate_area_v1')\n",
    "# len(dl_tag)\n",
    "a_tag = ul_tag.find_all('a')\n",
    "\n",
    "keyword_list=[]\n",
    "for keywords in a_tag:\n",
    "    #final_keywords = keywords.text\n",
    "    keyword_list.append(keywords.text)\n",
    "keyword_list\n",
    "#searches_list = []\n",
    "# li_tags =ul_tag.find_all('li', recursive=False)\n",
    "# for li_tag in li_tags :\n",
    "#     word =li_tag \n",
    "#     searches_list.appned(word)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연습문제(몇페이까지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 5\n",
    "page_num = int(input(\"몇페이지까지?\"))\n",
    "query = input(\"검색어 입력해 주세요.\")\n",
    "\n",
    "url = 'https://search.naver.com/search.naver'\n",
    "\n",
    "result_list = []\n",
    "for i in range(page_num):\n",
    "    params = {\n",
    "        'where': 'post',\n",
    "        'query': query,\n",
    "        'start': 1 + i*10\n",
    "    }\n",
    "    resp = requests.get(url, params=params)\n",
    "    \n",
    "    soup = BeautifulSoup(resp.content)\n",
    "    ul_tag = soup.find('ul', class_='type01')\n",
    "    \n",
    "    blog_list = ul_tag.find_all('li', class_='sh_blog_top')\n",
    "    \n",
    "    for post in blog_list:\n",
    "        dl_tag = post.find('dl')\n",
    "        title = dl_tag.find('a', class_='_sp_each_title').text\n",
    "        link = dl_tag.find('a', class_='_sp_each_title').get('href')\n",
    "        desc = dl_tag.find_all('dd')[1].text\n",
    "        \n",
    "        result_list.append({\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'desc': desc\n",
    "        })\n",
    "result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연습문제4\n",
    "-네이버 뉴스/블로그 상세페이지 조회하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://search.naver.com/search.naver'\n",
    "params={\n",
    "    'where':'news',\n",
    "    'query':\"카누\"\n",
    "}\n",
    "resp=requests.get(url, params=params)\n",
    "soup = BeautifulSoup(resp.content)\n",
    "\n",
    "ul_tag =soup.find('ul', class_='type01')\n",
    "news_tag =ul_tag.find_all('li',recursive=False)\n",
    "link_list=[]\n",
    "new_link_list=[]\n",
    "for news in news_tag:\n",
    "    a_tag = news.find('a', class_='_sp_each_title')\n",
    "    title = a_tag.find('a', class_='_sp_each_title')\n",
    "    link = a_tag.get('href')\n",
    "    desc = news.find_all('dd')[1].text\n",
    "    \n",
    "    link_list.append(link)\n",
    "\n",
    "    resp_link=requests.get(link)\n",
    "    soup_link= BeautifulSoup(resp_link.content)\n",
    "    body_tag = soup_link.find('body')\n",
    "    \n",
    "    new_link_list.append({\n",
    "        'title': title,\n",
    "        'link':link,\n",
    "        'link_body':body_tag\n",
    "    })\n",
    "\n",
    "new_link_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예외처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = int(input(\"입력 a\"))\n",
    "b = int(input(\"입력 b\"))\n",
    "\n",
    "try:\n",
    "    c = a/b\n",
    "except ZeroDivisionError:\n",
    "#     if b==0:\n",
    "#         b=1\n",
    "    b =1\n",
    "    c =a/b\n",
    "finally:\n",
    "    print(c)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 뉴스 날씨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://search.naver.com/search.naver'\n",
    "params={\n",
    "    'where':'news',\n",
    "    'query':'오늘+날씨'\n",
    "}\n",
    "resp=requests.get(url, params=params)\n",
    "soup = BeautifulSoup(resp.content)\n",
    "soup.text\n",
    "ul_tag =soup.find('ul', class_='type01')\n",
    "\n",
    "news_li_tag = ul_tag.find_all('li', recursive=False)\n",
    "\n",
    "my_news_list=[]\n",
    "\n",
    "for news_tag in news_li_tag:\n",
    "    a_tag = news_tag.find('a', class_='_sp_each_title')\n",
    "    title = a_tag.text\n",
    "    link = a_tag.get('href')\n",
    "    desc = news_tag.find_all('dd')[1].text\n",
    "    \n",
    "    my_news_list.append({\n",
    "        'title':title,\n",
    "        'link':link,\n",
    "        'desc':desc\n",
    "    })\n",
    "for idx, my_news in enumerate(my_news_list):\n",
    "    resp= requests.get(my_news.get('link'))\n",
    "    soup = BeautifulSoup(resp.content)\n",
    "    \n",
    "    my_news_list[idx]['content']=soup.find('body')\n",
    "my_news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습\n",
    "-네이버 삼성전자 주가 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://finance.naver.com/item/sise.nhn'\n",
    "params={\n",
    "    'code':'005930'\n",
    "    \n",
    "}\n",
    "resp = requests.get(url)\n",
    "soup = BeautifulSoup(resp.content)\n",
    "\n",
    "#내용없음\n",
    "tag = soup.find_all(class_='type2') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lframe주소로 요청"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://finance.naver.com/item/sise_day.nhn'\n",
    "params={\n",
    "    'code':'005930'\n",
    "    \n",
    "}\n",
    "resp = requests.get(url, params=params)\n",
    "soup = BeautifulSoup(resp.content)\n",
    "data = soup.find(class_='type2')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 실습\n",
    "\n",
    "-하나의 request는 일반적으로 하나의 frame을 만듬\n",
    "브라우저는 HTML을 읽어서 적절히 요청을 여러번 보냄\n",
    "->HTML을 읽어서 적절히 요철을 다시 보냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "now.strftime('%Y%m%d%H%M%S')\n",
    "_type =input('형식: (day or time):')\n",
    "stock_code =input(\"종목코드: \")\n",
    "page_num = int(input(\"페이지 (-1은 끝까지)\"))\n",
    "\n",
    "url ='https://finance.naver.com/item/sise_{0}.nhn'.format(_type)\n",
    "params={\n",
    "    'code' : stock_code,\n",
    "    'thistime' : now.strftime('%Y%m%d%H%M%S')\n",
    "}\n",
    "resp = requests.get(url, params=params)\n",
    "\n",
    "page_num=3\n",
    "\n",
    "\n",
    "if(page_num == -1):\n",
    "    page_num=10000000\n",
    "result_list=[]\n",
    "for i in range(1, page_num+1):\n",
    "    params['page']=i\n",
    "    resp = requests.get(url, params=params)\n",
    "    \n",
    "    soup = BeautifulSoup(resp.content)\n",
    "    stock_table = soup.find('table', class_='type2')\n",
    "    \n",
    "    tr_tags =stock_table.find_all('tr')\n",
    "    \n",
    "    for tr_tag in tr_tags:\n",
    "        td_tag = tr_tag.find_all('td')\n",
    "        \n",
    "        if len(td_tag) <7:\n",
    "            continue\n",
    "        result_list.append({\n",
    "            'time': td_tag[0].text,\n",
    "            'price': td_tag[1].text,\n",
    "            'relative': td_tag[2].text,\n",
    "            'sell': td_tag[3].text,\n",
    "            'buy': td_tag[4].text,\n",
    "            'trade_amount': td_tag[5].text,\n",
    "            'changing_amount': td_tag[6].text\n",
    "        })\n",
    "        if len(tr_tags) < 10:\n",
    "            break\n",
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
